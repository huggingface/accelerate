diff --git a/src/accelerate/utils/dataclasses.py b/src/accelerate/utils/dataclasses.py
index bdc78bb..a3f9930 100644
--- a/src/accelerate/utils/dataclasses.py
+++ b/src/accelerate/utils/dataclasses.py
@@ -2768,57 +2768,57 @@ def parse_llama_config(megatron_lm_plugin, model, batch_data):
     megatron_lm_plugin.megatron_lm_default_args["model_return_dict"] = model.config.return_dict
     logger.info(f"Parsed Llama config: {megatron_lm_plugin.megatron_lm_default_args}")
 
-@add_model_config_to_megatron_parser("qwen3")
-def parse_qwen3_config(megatron_lm_plugin, model, batch_data):
-    model_type_name = "gpt"
-    num_layers = model.config.num_hidden_layers
-    pretraining_flag = True
-    hidden_size = model.config.hidden_size
-    num_attention_heads = model.config.num_attention_heads
-    orig_vocab_size = model.config.vocab_size
-
-    max_position_embeddings = model.config.max_position_embeddings
-    seq_length = getattr(model.config, "max_sequence_length", None)
-    if megatron_lm_plugin.seq_length is None:
-        if seq_length is not None:
-            megatron_lm_plugin.seq_length = seq_length
-        elif megatron_lm_plugin.decoder_seq_length is not None:
-            megatron_lm_plugin.seq_length = megatron_lm_plugin.decoder_seq_length
-        elif batch_data is not None:
-            megatron_lm_plugin.seq_length = batch_data["input_ids"].shape[1]
-        else:
-            megatron_lm_plugin.seq_length = max_position_embeddings
-
-    megatron_lm_plugin.megatron_lm_default_args["return_logits"] = megatron_lm_plugin.return_logits
-    megatron_lm_plugin.megatron_lm_default_args["tokenizer_type"] = "Qwen3Tokenizer"
-    megatron_lm_plugin.megatron_lm_default_args["model_type_name"] = model_type_name
-    megatron_lm_plugin.megatron_lm_default_args["num_layers"] = num_layers
-    megatron_lm_plugin.megatron_lm_default_args["pretraining_flag"] = pretraining_flag
-    megatron_lm_plugin.megatron_lm_default_args["hidden_size"] = hidden_size
-    megatron_lm_plugin.megatron_lm_default_args["num_attention_heads"] = num_attention_heads
-    megatron_lm_plugin.megatron_lm_default_args["num_query_groups"] = model.config.num_key_value_heads
-    megatron_lm_plugin.megatron_lm_default_args["num_key_value_heads"] = model.config.num_key_value_heads
-    megatron_lm_plugin.megatron_lm_default_args["group_query_attention"] = True
-    megatron_lm_plugin.megatron_lm_default_args["orig_vocab_size"] = orig_vocab_size
-    megatron_lm_plugin.megatron_lm_default_args["max_position_embeddings"] = max_position_embeddings
-    megatron_lm_plugin.megatron_lm_default_args["seq_length"] = megatron_lm_plugin.seq_length
-    megatron_lm_plugin.megatron_lm_default_args["kv_channels"] = model.config.head_dim
-    megatron_lm_plugin.megatron_lm_default_args["head_dim"] = model.config.head_dim
-    megatron_lm_plugin.megatron_lm_default_args["model_return_dict"] = model.config.return_dict
-    megatron_lm_plugin.megatron_lm_default_args["add_bias_linear"] = False
-    megatron_lm_plugin.megatron_lm_default_args["add_qkv_bias"] = False
-    megatron_lm_plugin.megatron_lm_default_args["layernorm_zero_centered_gamma"] = False
-    megatron_lm_plugin.megatron_lm_default_args["position_embedding_type"] = 'rope'
-    megatron_lm_plugin.megatron_lm_default_args["normalization"] = 'RMSNorm'
-    megatron_lm_plugin.megatron_lm_default_args["qk_layernorm"] = True
-    megatron_lm_plugin.megatron_lm_default_args["ffn_hidden_size"] = model.config.intermediate_size
-    megatron_lm_plugin.megatron_lm_default_args["swiglu"] = True
-    # megatron_lm_plugin.megatron_lm_default_args["fp8"] = model.config.fp8
-    # megatron_lm_plugin.megatron_lm_default_args["fp8_param"] = model.config.fp8_param
-    # megatron_lm_plugin.megatron_lm_default_args["fp8_param_gather"] = model.config.fp8_param_gather
-    # megatron_lm_plugin.megatron_lm_default_args["fp8_recipe"] = model.config.fp8_recipe
-    megatron_lm_plugin.megatron_lm_default_args["bf16"] = model.config.bf16
-    logger.info(f"Parsed Qwen3 config: {megatron_lm_plugin.megatron_lm_default_args}")
+# @add_model_config_to_megatron_parser("qwen3")
+# def parse_qwen3_config(megatron_lm_plugin, model, batch_data):
+#     model_type_name = "gpt"
+#     num_layers = model.config.num_hidden_layers
+#     pretraining_flag = True
+#     hidden_size = model.config.hidden_size
+#     num_attention_heads = model.config.num_attention_heads
+#     orig_vocab_size = model.config.vocab_size
+
+#     max_position_embeddings = model.config.max_position_embeddings
+#     seq_length = getattr(model.config, "max_sequence_length", None)
+#     if megatron_lm_plugin.seq_length is None:
+#         if seq_length is not None:
+#             megatron_lm_plugin.seq_length = seq_length
+#         elif megatron_lm_plugin.decoder_seq_length is not None:
+#             megatron_lm_plugin.seq_length = megatron_lm_plugin.decoder_seq_length
+#         elif batch_data is not None:
+#             megatron_lm_plugin.seq_length = batch_data["input_ids"].shape[1]
+#         else:
+#             megatron_lm_plugin.seq_length = max_position_embeddings
+
+#     megatron_lm_plugin.megatron_lm_default_args["return_logits"] = megatron_lm_plugin.return_logits
+#     megatron_lm_plugin.megatron_lm_default_args["tokenizer_type"] = "Qwen3Tokenizer"
+#     megatron_lm_plugin.megatron_lm_default_args["model_type_name"] = model_type_name
+#     megatron_lm_plugin.megatron_lm_default_args["num_layers"] = num_layers
+#     megatron_lm_plugin.megatron_lm_default_args["pretraining_flag"] = pretraining_flag
+#     megatron_lm_plugin.megatron_lm_default_args["hidden_size"] = hidden_size
+#     megatron_lm_plugin.megatron_lm_default_args["num_attention_heads"] = num_attention_heads
+#     megatron_lm_plugin.megatron_lm_default_args["num_query_groups"] = model.config.num_key_value_heads
+#     megatron_lm_plugin.megatron_lm_default_args["num_key_value_heads"] = model.config.num_key_value_heads
+#     megatron_lm_plugin.megatron_lm_default_args["group_query_attention"] = True
+#     megatron_lm_plugin.megatron_lm_default_args["orig_vocab_size"] = orig_vocab_size
+#     megatron_lm_plugin.megatron_lm_default_args["max_position_embeddings"] = max_position_embeddings
+#     megatron_lm_plugin.megatron_lm_default_args["seq_length"] = megatron_lm_plugin.seq_length
+#     megatron_lm_plugin.megatron_lm_default_args["kv_channels"] = model.config.head_dim
+#     megatron_lm_plugin.megatron_lm_default_args["head_dim"] = model.config.head_dim
+#     megatron_lm_plugin.megatron_lm_default_args["model_return_dict"] = model.config.return_dict
+#     megatron_lm_plugin.megatron_lm_default_args["add_bias_linear"] = False
+#     megatron_lm_plugin.megatron_lm_default_args["add_qkv_bias"] = False
+#     megatron_lm_plugin.megatron_lm_default_args["layernorm_zero_centered_gamma"] = False
+#     megatron_lm_plugin.megatron_lm_default_args["position_embedding_type"] = 'rope'
+#     megatron_lm_plugin.megatron_lm_default_args["normalization"] = 'RMSNorm'
+#     megatron_lm_plugin.megatron_lm_default_args["qk_layernorm"] = True
+#     megatron_lm_plugin.megatron_lm_default_args["ffn_hidden_size"] = model.config.intermediate_size
+#     megatron_lm_plugin.megatron_lm_default_args["swiglu"] = True
+#     # megatron_lm_plugin.megatron_lm_default_args["fp8"] = model.config.fp8
+#     # megatron_lm_plugin.megatron_lm_default_args["fp8_param"] = model.config.fp8_param
+#     # megatron_lm_plugin.megatron_lm_default_args["fp8_param_gather"] = model.config.fp8_param_gather
+#     # megatron_lm_plugin.megatron_lm_default_args["fp8_recipe"] = model.config.fp8_recipe
+#     megatron_lm_plugin.megatron_lm_default_args["bf16"] = model.config.bf16
+#     logger.info(f"Parsed Qwen3 config: {megatron_lm_plugin.megatron_lm_default_args}")
 
 @add_model_config_to_megatron_parser("qwen3_moe")
 def parse_qwen3_moe_config(megatron_lm_plugin, model, batch_data):
@@ -2854,6 +2854,52 @@ def parse_qwen3_moe_config(megatron_lm_plugin, model, batch_data):
     megatron_lm_plugin.megatron_lm_default_args["seq_length"] = megatron_lm_plugin.seq_length
     megatron_lm_plugin.megatron_lm_default_args["model_return_dict"] = model.config.return_dict
     megatron_lm_plugin.megatron_lm_default_args["position_embedding_type"] = 'rope'
+
+    megatron_lm_plugin.megatron_lm_default_args["qk_layernorm"] = True # model.config.use_qk_norm  # this is true for glm4.5 but False for glm4.5-air.
+    megatron_lm_plugin.megatron_lm_default_args["add_bias_linear"] = False
+    megatron_lm_plugin.megatron_lm_default_args["group_query_attention"] = True
+    megatron_lm_plugin.megatron_lm_default_args["num_query_groups"] = model.config.num_key_value_heads
+    megatron_lm_plugin.megatron_lm_default_args["ffn_hidden_size"] = model.config.intermediate_size
+    megatron_lm_plugin.megatron_lm_default_args["add_qkv_bias"] = False
+    megatron_lm_plugin.megatron_lm_default_args["normalization"] = 'RMSNorm'
+    megatron_lm_plugin.megatron_lm_default_args["rotary-percent"] = 1.0
+    megatron_lm_plugin.megatron_lm_default_args["swiglu"] = True
+    megatron_lm_plugin.megatron_lm_default_args["moe_ffn_hidden_size"] = model.config.moe_intermediate_size
+    # megatron_lm_plugin.megatron_lm_default_args["moe_shared_expert_intermediate_size"] = model.config.moe_intermediate_size
+    megatron_lm_plugin.megatron_lm_default_args["moe_router_pre_softmax"] = False
+    megatron_lm_plugin.megatron_lm_default_args["moe_router_score_function"] = "softmax"
+    megatron_lm_plugin.megatron_lm_default_args["moe_router_enable_expert_bias"] = False
+    megatron_lm_plugin.megatron_lm_default_args["moe_router_bias_update_rate"] = 0
+    megatron_lm_plugin.megatron_lm_default_args["moe_router_load_balancing_type"] = "aux_loss"
+    megatron_lm_plugin.megatron_lm_default_args["moe_token_dispatcher_type"] = "alltoall"
+    megatron_lm_plugin.megatron_lm_default_args["moe_router_bias_update_rate"] = 0.001
+    norm_epsilon = 1e-4
+    megatron_lm_plugin.megatron_lm_default_args["norm_epsilon"] = norm_epsilon
+    megatron_lm_plugin.megatron_lm_default_args["moe_router_topk"] = model.config.num_experts_per_tok
+    # megatron_lm_plugin.megatron_lm_default_args["moe_router_topk_scaling_factor"] = model.config.router_aux_loss_coef
+    # megatron_lm_plugin.megatron_lm_default_args["moe_layer_freq"] = [0] * model.config.first_k_dense_replace + [1] * (model.config.num_hidden_layers - model.config.first_k_dense_replace)
+    megatron_lm_plugin.megatron_lm_default_args["num_experts"] = model.config.num_experts
+    megatron_lm_plugin.megatron_lm_default_args["moe_grouped_gemm"] = True
+    megatron_lm_plugin.megatron_lm_default_args["moe_router_dtype"] = "fp32"
+    megatron_lm_plugin.megatron_lm_default_args["moe_permute_fusion"] = True
+    megatron_lm_plugin.megatron_lm_default_args["moe_aux_loss_coeff"] = 0
+    megatron_lm_plugin.megatron_lm_default_args["rotary_base"] = model.config.rope_theta
+    megatron_lm_plugin.megatron_lm_default_args["rope_type"] = 'rope'
+    megatron_lm_plugin.megatron_lm_default_args["rotary_percent"] = 1.0 # model.config.partial_rotary_factor
+    megatron_lm_plugin.megatron_lm_default_args["decoder_last_pipeline_num_layers"] = 3
+    megatron_lm_plugin.megatron_lm_default_args["context_parallel_size"] = model.config.context_parallel_size
+    megatron_lm_plugin.megatron_lm_default_args["initial_loss_scale"] = 2**5
+    megatron_lm_plugin.megatron_lm_default_args["loss_scale"] = 2**5
+    megatron_lm_plugin.megatron_lm_default_args["use_flash_attn"] = True
+    megatron_lm_plugin.megatron_lm_default_args["eos_token_id"] = model.config.eos_token_id
+    # megatron_lm_plugin.megatron_lm_default_args["sequence_parallel"] = False
+    if model.config.fp8_param:
+        megatron_lm_plugin.megatron_lm_default_args["fp8"] = model.config.fp8
+        megatron_lm_plugin.megatron_lm_default_args["fp8_param"] = model.config.fp8_param
+        megatron_lm_plugin.megatron_lm_default_args["fp8_param_gather"] = model.config.fp8_param_gather
+        megatron_lm_plugin.megatron_lm_default_args["fp8_recipe"] = model.config.fp8_recipe
+    megatron_lm_plugin.megatron_lm_default_args["bf16"] = model.config.bf16
+    megatron_lm_plugin.megatron_lm_default_args["untie_embeddings_and_output_weights"] = not model.config.tie_word_embeddings  
     logger.info(f"Parsed Qwen3 MoE config: {megatron_lm_plugin.megatron_lm_default_args}")
 
 @add_model_config_to_megatron_parser("glm4_moe")
@@ -2919,11 +2965,18 @@ def parse_glm4_moe_config(megatron_lm_plugin, model, batch_data):
     megatron_lm_plugin.megatron_lm_default_args["rotary_base"] = model.config.rope_theta
     megatron_lm_plugin.megatron_lm_default_args["rope_type"] = 'rope'
     megatron_lm_plugin.megatron_lm_default_args["rotary_percent"] = model.config.partial_rotary_factor
-    megatron_lm_plugin.megatron_lm_default_args["decoder_last_pipeline_num_layers"] = 2
-    # megatron_lm_plugin.megatron_lm_default_args["fp8"] = model.config.fp8
-    # megatron_lm_plugin.megatron_lm_default_args["fp8_param"] = model.config.fp8_param
-    # megatron_lm_plugin.megatron_lm_default_args["fp8_param_gather"] = model.config.fp8_param_gather
-    # megatron_lm_plugin.megatron_lm_default_args["fp8_recipe"] = model.config.fp8_recipe
+    megatron_lm_plugin.megatron_lm_default_args["decoder_last_pipeline_num_layers"] = 4
+    megatron_lm_plugin.megatron_lm_default_args["context_parallel_size"] = model.config.context_parallel_size
+    # megatron_lm_plugin.megatron_lm_default_args["initial_loss_scale"] = 2**5
+    megatron_lm_plugin.megatron_lm_default_args["norm_epsilon"] = 1e-3
+    megatron_lm_plugin.megatron_lm_default_args["use_flash_attn"] = True
+    megatron_lm_plugin.megatron_lm_default_args["eos_token_id"] = model.config.eos_token_id
+    # megatron_lm_plugin.megatron_lm_default_args["sequence_parallel"] = False
+    if model.config.fp8_param:
+        megatron_lm_plugin.megatron_lm_default_args["fp8"] = model.config.fp8
+        megatron_lm_plugin.megatron_lm_default_args["fp8_param"] = model.config.fp8_param
+        megatron_lm_plugin.megatron_lm_default_args["fp8_param_gather"] = model.config.fp8_param_gather
+        megatron_lm_plugin.megatron_lm_default_args["fp8_recipe"] = model.config.fp8_recipe
     megatron_lm_plugin.megatron_lm_default_args["bf16"] = model.config.bf16
     megatron_lm_plugin.megatron_lm_default_args["untie_embeddings_and_output_weights"] = not model.config.tie_word_embeddings
     logger.info(f"Parsed GLM4 MoE config: {megatron_lm_plugin.megatron_lm_default_args}")
diff --git a/src/accelerate/utils/megatron_lm.py b/src/accelerate/utils/megatron_lm.py
index ecf0a1d..cd05a30 100644
--- a/src/accelerate/utils/megatron_lm.py
+++ b/src/accelerate/utils/megatron_lm.py
@@ -95,9 +95,9 @@ def model_provider_func(pre_process=True, post_process=True, add_encoder=True, a
     args.recompute_num_layers = 1
     # args.use_torch_fsdp2 = True
     args.use_custom_fsdp = True
-    args.sequence_parallel = True
+    args.sequence_parallel = False
     args.attention_backend = True
-    args.expert_model_parallel_size = 1
+    args.expert_model_parallel_size = 4
     # args.data_parallel_sharding_strategy = "optim_grads_params"
     mode = "pre-training" if args.pretraining_flag else "fine-tuning"
     logging.info(f"in model_provider_func with args: {args}")
@@ -608,10 +608,12 @@ class GPTTrainStep(AbstractTrainStep):
         self.get_batch = self.get_batch_func(accelerator, args.megatron_dataset_flag)
         self.loss_func = self.get_loss_func(accelerator)
         self.forward_step = self.get_forward_step_func()
-        self.eod_token = args.padded_vocab_size - 1
+        # self.eod_token = args.padded_vocab_size - 1
         if args.vocab_file is not None:
             tokenizer = get_tokenizer()
             self.eod_token = tokenizer.eod
+        self.eod_token = args.eos_token_id
+        self.pad_token = args.eos_token_id
         self.reset_position_ids = args.reset_position_ids
         self.reset_attention_mask = args.reset_attention_mask
         self.eod_mask_loss = args.eod_mask_loss
@@ -645,7 +647,7 @@ class GPTTrainStep(AbstractTrainStep):
             attention_mask, loss_mask, position_ids = get_ltor_masks_and_position_ids(
                 tokens, eod_token=self.eod_token, pad_token=self.eod_token, reset_position_ids=self.reset_position_ids, reset_attention_mask=self.reset_attention_mask, eod_mask_loss=self.eod_mask_loss, pad_mask_loss=True,
             )
-
+            # print(f"I am in get_batch_megatron, labels: {labels}, tokens: {tokens}, loss_mask: {loss_mask}, attention_mask: {attention_mask}, position_ids: {position_ids}")
             return tokens, labels, loss_mask, attention_mask, position_ids
 
         def get_batch_transformer(data_iterator):
@@ -662,6 +664,7 @@ class GPTTrainStep(AbstractTrainStep):
             attention_mask, loss_mask, position_ids = get_ltor_masks_and_position_ids(
                 tokens, eod_token=self.eod_token, pad_token=self.eod_token, reset_position_ids=self.reset_position_ids, reset_attention_mask=self.reset_attention_mask, eod_mask_loss=self.eod_mask_loss, pad_mask_loss=True,
             )
+            # print(f"I am in get_batch_transformer, labels: {labels}, tokens: {tokens}, loss_mask: {loss_mask}, attention_mask: {attention_mask}, position_ids: {position_ids}")
             return tokens, labels, loss_mask, attention_mask, position_ids
 
         if accelerator.state.megatron_lm_plugin.custom_get_batch_function is not None:
@@ -689,10 +692,12 @@ class GPTTrainStep(AbstractTrainStep):
             losses = losses.float()
             loss_mask = loss_mask.view(-1).float()
             if args.context_parallel_size > 1:
+                logging.info(f"in context_parallel_size > 1, loss value {torch.sum(losses.view(-1))} and loss_mask.sum(): {loss_mask.sum()}")
                 loss = torch.cat([torch.sum(losses.view(-1) * loss_mask).view(1), loss_mask.sum().view(1)])
                 torch.distributed.all_reduce(loss, group=mpu.get_context_parallel_group())
                 loss = loss[0] / loss[1]
             else:
+                # print(f"in context_parallel_size == 1, loss value {torch.sum(losses.view(-1))} and loss_mask.sum(): {loss_mask.sum()}")
                 loss = torch.sum(losses.view(-1) * loss_mask) / loss_mask.sum()
 
             # Check individual rank losses are not NaN prior to DP all-reduce.
@@ -722,6 +727,8 @@ class GPTTrainStep(AbstractTrainStep):
             tokens, labels, loss_mask, attention_mask, position_ids = self.get_batch(data_iterator)
             output_tensor = model(tokens, position_ids, attention_mask, labels=labels)
 
+            logging.info(f"gpt forward_step 729: output_tensor: {output_tensor.shape}, if has nan: {torch.isnan(output_tensor).any()}")
+            logging.info(f"gpt forward_step 731: output_tensor: {output_tensor.shape}, if has nan: {torch.isnan(output_tensor).any()}")
             return output_tensor, partial(self.loss_func, loss_mask)
 
         return forward_step
@@ -958,6 +965,8 @@ class MegatronEngine(torch.nn.Module):
         elif args.model_type_name == "bert":
             self.train_step_handler = BertTrainStep(accelerator, args)
         elif args.model_type_name == "gpt":
+            # args.eos_token_id = model[0].config.eos_token_id[0]
+            print(f"I am in MegatronEngine, args.eos_token_id: {args.eos_token_id}")
             self.train_step_handler = GPTTrainStep(accelerator, args)
         elif args.model_type_name == "t5":
             self.train_step_handler = T5TrainStep(accelerator, args)
